{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamangirkhan/Data110/blob/main/ArashNateghian_Project4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc7056ae-1e74-4fbb-a349-cc370d8cbacc",
      "metadata": {
        "id": "fc7056ae-1e74-4fbb-a349-cc370d8cbacc"
      },
      "source": [
        "Arash Nateghian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15189788-d8e7-4c7e-9d62-311291bd6c8e",
      "metadata": {
        "id": "15189788-d8e7-4c7e-9d62-311291bd6c8e",
        "outputId": "731ce078-0bb0-48cf-b875-137f3542427a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================================================\n",
            "Address Analyzer — Menu\n",
            "========================================================================\n",
            "1) Analyze .txt file(s)\n",
            "2) Show session summary\n",
            "3) Quit\n",
            "4) Analyze .txt files from a folder\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            ">  4\n",
            "Enter folder path:  D:\\MC\\PY\\64B\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files found:\n",
            "  1) Barack Obama 1st.txt\n",
            "  2) Barack Obama 2nd.txt\n",
            "  3) Donald J Trump 1st.txt\n",
            "  4) Donald J Trump 2nd.txt\n",
            "  5) George Bush.txt\n",
            "  6) George W. Bush 1st.txt\n",
            "  7) George W. Bush 2nd.txt\n",
            "  8) Joseph R Biden Jr.txt\n",
            "  9) Ronald Reagan 1st.txt\n",
            "  10) Ronald Reagan 2nd.txt\n",
            "  11) William J Clinton 2nd.txt\n",
            "  12) William J. Clinton 1st.txt\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Enter numbers (comma) or 'a' for all:  1,3,5,7\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================================================\n",
            "Analysis: Barack Obama 1st.txt\n",
            "------------------------------------------------------------------------\n",
            "Total words:           2417\n",
            "Sentence count:        121\n",
            "Avg words/sentence:    19.98\n",
            "\n",
            "Top 7 words (ex stopwords & numbers):\n",
            "  nation           12\n",
            "  new              11\n",
            "  america          10\n",
            "  people           7\n",
            "  less             7\n",
            "  world            7\n",
            "  work             6\n",
            "\n",
            "Readability:\n",
            "Flesch–Kincaid Grade: 9.80\n",
            "This means the text is written at about a US grade 10 reading level.\n",
            "Higher scores = harder text. Lower scores = easier to read.\n",
            "This helps judge how understandable a speech or essay is for the general public.\n",
            "\n",
            "Word cloud saved:      C:\\Users\\arash\\output\\barack-obama-1st-txt_wordcloud.png\n",
            "Top-words CSV:         C:\\Users\\arash\\output\\barack-obama-1st-txt_top_words.csv\n",
            "\n",
            "========================================================================\n",
            "Analysis: Donald J Trump 1st.txt\n",
            "------------------------------------------------------------------------\n",
            "Total words:           1487\n",
            "Sentence count:        90\n",
            "Avg words/sentence:    16.52\n",
            "\n",
            "Top 7 words (ex stopwords & numbers):\n",
            "  america          20\n",
            "  american         11\n",
            "  people           10\n",
            "  country          9\n",
            "  nation           8\n",
            "  president        6\n",
            "  world            6\n",
            "\n",
            "Readability:\n",
            "Flesch–Kincaid Grade: 9.45\n",
            "This means the text is written at about a US grade 9 reading level.\n",
            "Higher scores = harder text. Lower scores = easier to read.\n",
            "This helps judge how understandable a speech or essay is for the general public.\n",
            "\n",
            "Word cloud saved:      C:\\Users\\arash\\output\\donald-j-trump-1st-txt_wordcloud.png\n",
            "Top-words CSV:         C:\\Users\\arash\\output\\donald-j-trump-1st-txt_top_words.csv\n",
            "\n",
            "========================================================================\n",
            "Analysis: George Bush.txt\n",
            "------------------------------------------------------------------------\n",
            "Total words:           2356\n",
            "Sentence count:        153\n",
            "Avg words/sentence:    15.4\n",
            "\n",
            "Top 7 words (ex stopwords & numbers):\n",
            "  new              14\n",
            "  nation           10\n",
            "  great            10\n",
            "  world            10\n",
            "  free             9\n",
            "  friends          8\n",
            "  things           8\n",
            "\n",
            "Readability:\n",
            "Flesch–Kincaid Grade: 6.83\n",
            "This means the text is written at about a US grade 7 reading level.\n",
            "Higher scores = harder text. Lower scores = easier to read.\n",
            "This helps judge how understandable a speech or essay is for the general public.\n",
            "\n",
            "Word cloud saved:      C:\\Users\\arash\\output\\george-bush-txt_wordcloud.png\n",
            "Top-words CSV:         C:\\Users\\arash\\output\\george-bush-txt_top_words.csv\n",
            "\n",
            "========================================================================\n",
            "Analysis: George W. Bush 2nd.txt\n",
            "------------------------------------------------------------------------\n",
            "Total words:           2099\n",
            "Sentence count:        99\n",
            "Avg words/sentence:    21.2\n",
            "\n",
            "Top 7 words (ex stopwords & numbers):\n",
            "  freedom          27\n",
            "  america          20\n",
            "  liberty          15\n",
            "  nation           10\n",
            "  country          8\n",
            "  world            8\n",
            "  americans        8\n",
            "\n",
            "Readability:\n",
            "Flesch–Kincaid Grade: 11.04\n",
            "This means the text is written at about a US grade 11 reading level.\n",
            "Higher scores = harder text. Lower scores = easier to read.\n",
            "This helps judge how understandable a speech or essay is for the general public.\n",
            "\n",
            "Word cloud saved:      C:\\Users\\arash\\output\\george-w-bush-2nd-txt_wordcloud.png\n",
            "Top-words CSV:         C:\\Users\\arash\\output\\george-w-bush-2nd-txt_top_words.csv\n",
            "\n",
            "========================================================================\n",
            "Address Analyzer — Menu\n",
            "========================================================================\n",
            "1) Analyze .txt file(s)\n",
            "2) Show session summary\n",
            "3) Quit\n",
            "4) Analyze .txt files from a folder\n",
            "\n",
            "Interrupted. Bye.\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import os, re, sys, glob, textwrap, csv\n",
        "\n",
        "# --- Optional libraries (used if available) ---\n",
        "try:\n",
        "    from wordcloud import WordCloud\n",
        "except Exception:\n",
        "    WordCloud = None  # type: ignore\n",
        "\n",
        "try:\n",
        "    from readability import Readability\n",
        "    _HAS_READABILITY = True\n",
        "except Exception:\n",
        "    _HAS_READABILITY = False\n",
        "\n",
        "try:\n",
        "    import textstat  # fallback only\n",
        "except Exception:\n",
        "    textstat = None  # type: ignore\n",
        "\n",
        "# --- Output directory (works in script or notebook) ---\n",
        "try:\n",
        "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "except NameError:\n",
        "    BASE_DIR = os.getcwd()\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Simple English stopwords (lowercase) ---\n",
        "STOPWORDS = set(\n",
        "    \"\"\"\n",
        "    a about above after again against all am an and any are aren't as at be because been before being\n",
        "    below between both but by can can't cannot could couldn't did didn't do does doesn't doing don't down during each every\n",
        "    few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers herself\n",
        "    him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let let's me more most much must mustn't\n",
        "    my myself no nor not now of off on one once only or other ought our ours ourselves out over own re s same shan't she she'd\n",
        "    she'll she's should shouldn't so some such t thank than that that's the their theirs them themselves then there there's\n",
        "    these they they'd they'll they're they've this those through time to today together too under until up us very was wasn't we we'd we'll\n",
        "    we're we've were weren't what what's when when's where where's which while will who who's whom why why's with won't\n",
        "    would wouldn't you you'd you'll you're you've your yours yourself yourselves\n",
        "    \"\"\".split()\n",
        ")\n",
        "\n",
        "# --- Regex helpers ---\n",
        "WORD_RE = re.compile(r\"[A-Za-z]+(?:-[A-Za-z]+)*|\\d+\")  # words (keep hyphens) or digits\n",
        "POSSESSIVE_RE = re.compile(r\"(.*?)('s|’s)$\", re.IGNORECASE)\n",
        "SENT_SPLIT_RE = re.compile(r\"(?<=[.!?])\\s+\")\n",
        "\n",
        "@dataclass\n",
        "class AnalysisResult:\n",
        "    name: str\n",
        "    total_words: int\n",
        "    sentence_count: int\n",
        "    avg_sentence_len: float\n",
        "    top_words: List[Tuple[str, int]]\n",
        "    readability: Dict[str, float]\n",
        "    wordcloud_path: Optional[str]\n",
        "    top_words_csv_path: Optional[str]\n",
        "\n",
        "# --- Text processing ---\n",
        "\n",
        "def strip_possessive(token: str) -> str:\n",
        "    m = POSSESSIVE_RE.match(token)\n",
        "    return m.group(1) if m else token\n",
        "\n",
        "\n",
        "def tokenize_words(text: str) -> List[str]:\n",
        "    tokens = WORD_RE.findall(text.lower())\n",
        "    cleaned: List[str] = []\n",
        "    for t in tokens:\n",
        "        if t.isdigit():\n",
        "            cleaned.append(t)\n",
        "        else:\n",
        "            cleaned.append(strip_possessive(t))\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    return [p.strip() for p in SENT_SPLIT_RE.split(text) if p.strip()]\n",
        "\n",
        "# --- Core analytics ---\n",
        "\n",
        "def compute_stats(text: str, stopwords: set, top_n: int = 7):\n",
        "    sentences = split_sentences(text)\n",
        "    words = tokenize_words(text)\n",
        "    # sentence lengths by re-using same tokenizer\n",
        "    sent_lens = [len(tokenize_words(s)) for s in sentences]\n",
        "    total_words = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "    avg_sentence_len = round((sum(sent_lens)/sentence_count) if sentence_count else 0.0, 2)\n",
        "    content_words = [w for w in words if (not w.isdigit()) and (w not in stopwords)]\n",
        "    freq = Counter(content_words)\n",
        "    top = freq.most_common(top_n)\n",
        "    return total_words, sentence_count, avg_sentence_len, top, freq\n",
        "\n",
        "\n",
        "def generate_wordcloud(freq: Counter, out_path: str) -> Optional[str]:\n",
        "    if WordCloud is None:\n",
        "        return None\n",
        "    try:\n",
        "        wc = WordCloud(width=1200, height=700, background_color=\"white\")\n",
        "        wc.generate_from_frequencies(dict(freq))\n",
        "        wc.to_file(out_path)\n",
        "        return out_path\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def compute_readability(text: str) -> Dict[str, float]:\n",
        "    scores: Dict[str, float] = {}\n",
        "    if _HAS_READABILITY:\n",
        "        try:\n",
        "            r = Readability(text)\n",
        "            scores[\"flesch_kincaid_grade\"] = float(r.flesch_kincaid().score)\n",
        "            return scores\n",
        "        except Exception:\n",
        "            pass\n",
        "    if textstat is not None:\n",
        "        try:\n",
        "            scores[\"flesch_kincaid_grade\"] = float(textstat.flesch_kincaid_grade(text))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return scores\n",
        "\n",
        "# --- Utilities ---\n",
        "\n",
        "def slugify(name: str) -> str:\n",
        "    s = re.sub(r\"[^a-zA-Z0-9_-]+\", \"-\", name.strip())\n",
        "    s = re.sub(r\"-+\", \"-\", s).strip(\"-\")\n",
        "    return s.lower() or \"address\"\n",
        "\n",
        "\n",
        "def read_text_file(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def save_top_words_csv(slug: str, top: List[Tuple[str, int]]) -> str:\n",
        "    path = os.path.join(OUTPUT_DIR, f\"{slug}_top_words.csv\")\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"word\", \"count\"])\n",
        "        for word, cnt in top:\n",
        "            w.writerow([word, cnt])\n",
        "    return path\n",
        "\n",
        "\n",
        "def print_report(res: AnalysisResult) -> None:\n",
        "    print(\"\\n\" + \"=\"*72)\n",
        "    print(f\"Analysis: {res.name}\")\n",
        "    print(\"-\"*72)\n",
        "    print(f\"Total words:           {res.total_words}\")\n",
        "    print(f\"Sentence count:        {res.sentence_count}\")\n",
        "    print(f\"Avg words/sentence:    {res.avg_sentence_len}\")\n",
        "    print(\"\\nTop 7 words (ex stopwords & numbers):\")\n",
        "    if not res.top_words:\n",
        "        print(\"  (none)\")\n",
        "    else:\n",
        "        for w, c in res.top_words:\n",
        "            print(f\"  {w:<16} {c}\")\n",
        "    print(\"\\nReadability:\")\n",
        "    if not res.readability:\n",
        "        print(\"  (readability libraries unavailable)\")\n",
        "    else:\n",
        "        fk = res.readability.get(\"flesch_kincaid_grade\")\n",
        "        if fk is not None:\n",
        "            print(\n",
        "        f\"Flesch–Kincaid Grade: {fk:.2f}\\n\"\n",
        "        f\"This means the text is written at about a US grade {round(fk)} reading level.\\n\"\n",
        "        \"Higher scores = harder text. Lower scores = easier to read.\\n\"\n",
        "        \"This helps judge how understandable a speech or essay is for the general public.\"\n",
        "    )\n",
        "    if res.wordcloud_path:\n",
        "        print(f\"\\nWord cloud saved:      {res.wordcloud_path}\")\n",
        "    else:\n",
        "        print(\"\\nWord cloud:            (skipped — install 'wordcloud')\")\n",
        "    if res.top_words_csv_path:\n",
        "        print(f\"Top-words CSV:         {res.top_words_csv_path}\")\n",
        "\n",
        "# --- Folder/file selection ---\n",
        "\n",
        "def choose_from_folder(folder: str) -> List[str]:\n",
        "    txts = glob.glob(os.path.join(folder, \"*.txt\"))\n",
        "    if not txts:\n",
        "        print(\"No .txt files found.\")\n",
        "        return []\n",
        "    print(\"Files found:\")\n",
        "    for i, p in enumerate(txts, 1):\n",
        "        print(f\"  {i}) {os.path.basename(p)}\")\n",
        "    sel = input(\"Enter numbers (comma) or 'a' for all: \").strip().lower()\n",
        "    if sel == \"a\":\n",
        "        return txts\n",
        "    idxs = []\n",
        "    for s in sel.split(','):\n",
        "        s = s.strip()\n",
        "        if not s:\n",
        "            continue\n",
        "        try:\n",
        "            idxs.append(int(s)-1)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return [txts[i] for i in idxs if 0 <= i < len(txts)]\n",
        "\n",
        "\n",
        "def pick_files_prompt() -> List[str]:\n",
        "    raw = input(\"\\nEnter .txt paths (comma-separated), or press ENTER to cancel:\\n> \").strip()\n",
        "    if not raw:\n",
        "        return []\n",
        "    paths = [p.strip().strip('\"\\'') for p in raw.split(',')]\n",
        "    return [p for p in paths if os.path.isfile(p)]\n",
        "\n",
        "# --- Main analysis wrapper ---\n",
        "\n",
        "def analyze_text(name: str, text: str, stopwords: set) -> AnalysisResult:\n",
        "    total_words, scount, avg_len, top, freq = compute_stats(text, stopwords)\n",
        "    slug = slugify(name)\n",
        "    wc_path = os.path.join(OUTPUT_DIR, f\"{slug}_wordcloud.png\")\n",
        "    wc_saved = generate_wordcloud(freq, wc_path)\n",
        "    top_csv = save_top_words_csv(slug, top)\n",
        "    scores = compute_readability(text)\n",
        "    return AnalysisResult(\n",
        "        name=name,\n",
        "        total_words=total_words,\n",
        "        sentence_count=scount,\n",
        "        avg_sentence_len=avg_len,\n",
        "        top_words=top,\n",
        "        readability=scores,\n",
        "        wordcloud_path=wc_saved,\n",
        "        top_words_csv_path=top_csv,\n",
        "    )\n",
        "\n",
        "# --- Menu loop ---\n",
        "\n",
        "def menu_loop():\n",
        "    stops = STOPWORDS\n",
        "    analyzed: List[AnalysisResult] = []\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*72)\n",
        "        print(\"Address Analyzer — Menu\")\n",
        "        print(\"=\"*72)\n",
        "        print(\"1) Analyze .txt file(s)\")\n",
        "        print(\"2) Show session summary\")\n",
        "        print(\"3) Quit\")\n",
        "        print(\"4) Analyze .txt files from a folder\")\n",
        "        choice = input(\"> \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            paths = pick_files_prompt()\n",
        "            if not paths:\n",
        "                continue\n",
        "            for p in paths:\n",
        "                try:\n",
        "                    text = read_text_file(p)\n",
        "                except Exception as e:\n",
        "                    print(f\"  ! Failed to read {p}: {e}\")\n",
        "                    continue\n",
        "                name = os.path.basename(p)\n",
        "                res = analyze_text(name, text, stops)\n",
        "                analyzed.append(res)\n",
        "                print_report(res)\n",
        "                # append to session CSV\n",
        "                row = {\n",
        "                    \"name\": res.name,\n",
        "                    \"total_words\": str(res.total_words),\n",
        "                    \"sentence_count\": str(res.sentence_count),\n",
        "                    \"avg_sentence_len\": f\"{res.avg_sentence_len}\",\n",
        "                    \"fk_grade\": f\"{res.readability.get('flesch_kincaid_grade', '')}\",\n",
        "                    \"wordcloud_path\": res.wordcloud_path or \"\",\n",
        "                }\n",
        "                path_csv = os.path.join(OUTPUT_DIR, \"session_summary.csv\")\n",
        "                write_header = not os.path.exists(path_csv)\n",
        "                with open(path_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                    w = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
        "                    if write_header:\n",
        "                        w.writeheader()\n",
        "                    w.writerow(row)\n",
        "\n",
        "        elif choice == \"2\":  # summary\n",
        "            if not analyzed:\n",
        "                print(\"No analyses yet.\")\n",
        "                continue\n",
        "            print(\"\\nSession summary (most recent first):\")\n",
        "            for res in reversed(analyzed):\n",
        "                top1 = res.top_words[0][0] if res.top_words else \"—\"\n",
        "                print(f\"- {res.name}: words={res.total_words}, avg_wps={res.avg_sentence_len}, top1={top1}\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            print(\"Bye.\")\n",
        "            break\n",
        "\n",
        "        elif choice == \"4\":  # folder flow\n",
        "            folder = input(\"Enter folder path: \").strip()\n",
        "            if not os.path.isdir(folder):\n",
        "                print(\"Invalid folder.\")\n",
        "                continue\n",
        "            paths = choose_from_folder(folder)\n",
        "            if not paths:\n",
        "                print(\"No files selected.\")\n",
        "                continue\n",
        "            for p in paths:\n",
        "                try:\n",
        "                    text = read_text_file(p)\n",
        "                except Exception as e:\n",
        "                    print(f\"  ! Failed to read {p}: {e}\")\n",
        "                    continue\n",
        "                name = os.path.basename(p)\n",
        "                res = analyze_text(name, text, stops)\n",
        "                analyzed.append(res)\n",
        "                print_report(res)\n",
        "                # log as above\n",
        "                row = {\n",
        "                    \"name\": res.name,\n",
        "                    \"total_words\": str(res.total_words),\n",
        "                    \"sentence_count\": str(res.sentence_count),\n",
        "                    \"avg_sentence_len\": f\"{res.avg_sentence_len}\",\n",
        "                    \"fk_grade\": f\"{res.readability.get('flesch_kincaid_grade', '')}\",\n",
        "                    \"wordcloud_path\": res.wordcloud_path or \"\",\n",
        "                }\n",
        "                path_csv = os.path.join(OUTPUT_DIR, \"session_summary.csv\")\n",
        "                write_header = not os.path.exists(path_csv)\n",
        "                with open(path_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                    w = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
        "                    if write_header:\n",
        "                        w.writeheader()\n",
        "                    w.writerow(row)\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Try 1-4.\")\n",
        "\n",
        "# --- Entry point ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        menu_loop()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nInterrupted. Bye.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6023cd-9efd-405f-b963-d38d66bd0a98",
      "metadata": {
        "id": "fe6023cd-9efd-405f-b963-d38d66bd0a98"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}