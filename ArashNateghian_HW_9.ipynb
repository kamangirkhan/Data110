{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamangirkhan/Data110/blob/main/ArashNateghian_HW_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2876c8f-fc64-4c34-ba6f-015f3fe5de2b",
      "metadata": {
        "id": "b2876c8f-fc64-4c34-ba6f-015f3fe5de2b"
      },
      "source": [
        "Arash Nateghian"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15b2a3d3-acc4-4730-8a8c-d5a68f87c451",
      "metadata": {
        "id": "15b2a3d3-acc4-4730-8a8c-d5a68f87c451"
      },
      "source": [
        "1.\tComplete the following tutorial:\n",
        "  \n",
        "o\tHow to get Rank of page in google search results using BeautifulSoup\n",
        "\n",
        "▪\thttps://www.geeksforgeeks.org/how-to-get-rank-of-page-in-google-search-results-using-beautifulsoup/\n",
        "\n",
        "▪\tYou may need to import / install the BeautifulSoup library first using pip (or !pip for Colab)\n",
        "\n",
        "o\tInclude one additional enhancement of your choice\n",
        "\n",
        "o\tComplete the tutorial as best as you can and share your learning experience.  That’s what I am looking for\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "30e26d83-d590-43f5-b414-74417843bab9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30e26d83-d590-43f5-b414-74417843bab9",
        "outputId": "499e3830-ffff-40f1-ff15-566d7566b41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ccddfca3-89dd-441a-b442-9e5fc264341f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccddfca3-89dd-441a-b442-9e5fc264341f",
        "outputId": "27b59408-3162-4cfc-bbaa-6e36a8b6e04e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Query: best data science course\n",
            "Target Site: coursera.org\n",
            "Rank in top results: None\n",
            "Total search results: N/A\n",
            "\n",
            "Top Organic Results:\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def google_search_rank(query, target_domain, top_n=10):\n",
        "    query = query.replace(\" \", \"+\")\n",
        "    url = f\"https://www.google.com/search?q={query}\"\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Scrape organic result blocks\n",
        "    results = soup.find_all(\"div\", class_=\"yuRUbf\")\n",
        "\n",
        "    rank = None\n",
        "    organic_results = []\n",
        "\n",
        "    for i, result in enumerate(results[:top_n]):\n",
        "        link = result.a[\"href\"]\n",
        "        organic_results.append(link)\n",
        "\n",
        "        if target_domain in link:\n",
        "            rank = i + 1  # rank position (1-indexed)\n",
        "\n",
        "    # Enhancement: extract \"About X results\"\n",
        "    stat_div = soup.find(\"div\", id=\"result-stats\")\n",
        "    total_results = stat_div.text if stat_div else \"N/A\"\n",
        "\n",
        "    return rank, organic_results, total_results\n",
        "\n",
        "\n",
        "# Test\n",
        "query = \"best data science course\"\n",
        "domain = \"coursera.org\"\n",
        "\n",
        "rank, results, total = google_search_rank(query, domain)\n",
        "\n",
        "print(f\"Search Query: {query}\")\n",
        "print(f\"Target Site: {domain}\")\n",
        "print(f\"Rank in top results: {rank}\")\n",
        "print(\"Total search results:\", total)\n",
        "print(\"\\nTop Organic Results:\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"{i}. {r}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e555fb2-5352-4309-ae3e-33d163ff0344",
      "metadata": {
        "id": "9e555fb2-5352-4309-ae3e-33d163ff0344"
      },
      "source": [
        "**What I learned**\n",
        "\n",
        "Google does not provide clean HTML; you must inspect tags manually.\n",
        "\n",
        "Adding a User-Agent header is required, otherwise Google blocks the request.\n",
        "\n",
        "Organic search results are found inside div.yuRUbf blocks.\n",
        "\n",
        "**Challenges**\n",
        "\n",
        "Google changes HTML structure often, so scraping breaks easily."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5e50a37-194e-4d95-a35c-aeea6994d659",
      "metadata": {
        "id": "b5e50a37-194e-4d95-a35c-aeea6994d659"
      },
      "source": [
        "2.\tOptional – Complete the following tutorial:\n",
        "  \n",
        "o\tLet’s build a search engine with python\n",
        "\n",
        "▪\thttps://blog.devgenius.io/lets-build-a-search-engine-with-python-3f8dd3320210"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78788046-9667-471c-859e-e353aa90d664",
      "metadata": {
        "id": "78788046-9667-471c-859e-e353aa90d664"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    return re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "def build_index(folder):\n",
        "    index = defaultdict(set)\n",
        "\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            path = os.path.join(folder, filename)\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
        "                words = tokenize(file.read())\n",
        "                for word in words:\n",
        "                    index[word].add(filename)\n",
        "\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd2d851-91d2-4a89-88a3-c195e0caa1e4",
      "metadata": {
        "id": "ebd2d851-91d2-4a89-88a3-c195e0caa1e4"
      },
      "outputs": [],
      "source": [
        "def search(index, query):\n",
        "    words = tokenize(query)\n",
        "    results = None\n",
        "\n",
        "    for word in words:\n",
        "        if word in index:\n",
        "            if results is None:\n",
        "                results = index[word]\n",
        "            else:\n",
        "                results = results.intersection(index[word])\n",
        "        else:\n",
        "            return set()\n",
        "\n",
        "    return results if results else set()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05d0ecd-b6ed-4f58-9a88-d83a94ef8875",
      "metadata": {
        "id": "a05d0ecd-b6ed-4f58-9a88-d83a94ef8875",
        "outputId": "a7ce9a1f-bb5a-48b3-807b-86112ea93700"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "\n",
            "Search:  obama\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results: {'Barack Obama 1st.txt', 'Barack Obama 2nd.txt', 'Donald J Trump 1st.txt', 'Donald J Trump 2nd.txt'}\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "\n",
            "Search:  american\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results: {'Donald J Trump 1st.txt', 'Barack Obama 2nd.txt', 'Ronald Reagan 1st.txt', 'Ronald Reagan 2nd.txt', 'George Bush.txt', 'George W. Bush 1st.txt', 'Donald J Trump 2nd.txt', 'George W. Bush 2nd.txt', 'Barack Obama 1st.txt', 'William J. Clinton 1st.txt', 'William J Clinton 2nd.txt', 'Joseph R Biden Jr.txt'}\n"
          ]
        }
      ],
      "source": [
        "folder = \"docs\"\n",
        "index = build_index(folder)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nSearch: \")\n",
        "    if query.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    results = search(index, query)\n",
        "    print(\"Results:\", results if results else \"No matches found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791d291d-0b23-4ae9-a827-6590508ef78b",
      "metadata": {
        "id": "791d291d-0b23-4ae9-a827-6590508ef78b"
      },
      "source": [
        "In this exercise, I built a simple text-based search engine in Python using an inverted index. I learned how search engines scan files ahead of time, tokenize text into searchable terms, and store document references so lookup becomes fast. The biggest concept shift for me was understanding why indexing matters — without it, searching every document each time would be inefficient.\n",
        "\n",
        "One challenge was implementing the search logic so it correctly handles multiple-word queries using set intersections. This forced me to think more about data structures and the flow of information rather than just writing code line by line.\n",
        "\n",
        "For my enhancement, I extended the basic search by adding ranking logic based on word frequency, so results are not only matched but scored for relevance. Overall, this project gave me a clearer view of the fundamental components behind search technology, even in a simplified form."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}